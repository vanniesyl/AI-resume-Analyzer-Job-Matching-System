{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x4Ei1zyd-7L",
        "outputId": "2340fcf4-0596-44af-b82c-3e322652609f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy sklearn PyPDF2 nltk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy sklearn PyPDF2 nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzuAITqYeJIT",
        "outputId": "6837ae83-ca76-4530-c438-535e18e48415"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYLPcpj8eUkl",
        "outputId": "6af79a50-f6eb-46ce-df85-afb8f1015d21"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by482X8peXHw",
        "outputId": "f839c45f-9023-4faa-f953-5dcdf2253f6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_resume_text(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in range(len(reader.pages)):\n",
        "            text += reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "resume_text = extract_resume_text('sample_resume.pdf')\n",
        "print(resume_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ZbIbqodLeaTW",
        "outputId": "244b27b0-65d5-4b18-d806-027e9afaae91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'PyPDF2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-021595f0d078>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_resume_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFgl4YoVfJGB",
        "outputId": "17c1e14a-4415-4050-bf03-68a0789f8faf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n"
      ],
      "metadata": {
        "id": "h5M4uZyqfMeV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_resume_text(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in range(len(reader.pages)):\n",
        "            text += reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "resume_text = extract_resume_text('/content/CV update september.pdf')\n",
        "print(resume_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H79-wM4ifO0s",
        "outputId": "7afd9a4e-1435-4303-d053-292ad0ea1ecf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fahad Hossain\n",
            "/envel⌢pefahadhossain1139@gmail.com ♂¶ap-¶arker-alt11355 SW Sweetwater 2nd Street, Miami, Florida\n",
            "ὑ7fahad16301139.github.io ♂phone-alt718-666-0971\n",
            "Education\n",
            "MS in Computer Science , Florida International University Jan 2023 – Dec 2024\n",
            "CGPA: 3.74\n",
            "B.Sc. in Computer Science and Engineering , BRAC University Jan 2016 – Dec 2020\n",
            "CGPA: 3.20\n",
            "Work Experience\n",
            "Teaching Assistant , Florida International University, Miami Jan 2024-Present\n",
            "-Led interactive lab sessions, guiding students through hands-on experience in system-level programming and OS con-\n",
            "cepts\n",
            "- Assisted students in understanding the practical applications of programming in real-life scenarios and potential career\n",
            "paths.\n",
            "- Conducted tutorials to bolster programming proficiency and problem-solving skills.\n",
            "Data Analyst , Florida International University, Miami Oct-Dec 2023\n",
            "- Analyzed IT vs CS students’ work experiences to outline career paths using Python.\n",
            "- Presented findings to the team/supervisor, offering insights into the unique career trajectories and opportunities associ-\n",
            "ated with each degree.\n",
            "- Utilized PostgreSQL, Power BI, and Python for data processing, visualization, and interpretation.\n",
            "Volunteer Research Assistant , Florida International University May-Jul 2023\n",
            "- Led a XGBoost and GRU project for anomaly detection with 96.57% accuracy. Innovated feature extraction techniques for\n",
            "machine learning models.\n",
            "Thesis Research Assistant , BRAC University, Dhaka Feb-Dec 2020\n",
            "- Developed machine learning algorithms with a team, contributing to an IEEE publication on our findings and method-\n",
            "ologies.\n",
            "Projects\n",
            "Calorie Estimation System\n",
            "Led the development of a deep learning system using CNN, ResNet, and Inception V3 for food detection and calorie esti-\n",
            "mation. Demonstrated potential in health and nutrition. [Project Link] /external-link-alt\n",
            "Car Rental System\n",
            "Developed a PHP and MySQL based system to streamline car rental processes for users and administrators. [Project Link] /external-link-alt\n",
            "gunshot detector app\n",
            "Developed an innovative Flask-based API for real-time gunshot detection using machine learning, feature extraction, and\n",
            "a pre-trained model, featuring a user-friendly interface for audio file uploads and immediate results. [Project Link] /external-link-alt\n",
            "Publications\n",
            "FoodieCal: A Convolutional Neural Network Based Food Detection and Calorie Estimation System\n",
            "S. A. Ayon, C. Z. Mashrafi, A. B. Yousuf, F. Hossain, and M. I. Hossain. 2021 National Computing Colleges Conference (NCCC),\n",
            "2021, pp. 1-6. DOI: 10.1109/NCCC49330.2021.9428820 /external-link-alt\n",
            "Hybrid machine learning approach for improved Ponzi Scheme detection using advanced feature engineering\n",
            "Hossain, Fahad; UDDIN, JIA. Florida International University, Department of Computer and Information Science. Status:\n",
            "In Submission.\n",
            "Skills\n",
            "Programming Languages: Java, PHP , SQL, Python, HTML/CSS\n",
            "Digital Skills: Microsoft Word, PowerPoint, Excel, LaTeX, Google Colab\n",
            "Fahad Hossain - Page 1 of 1Last updated in February 2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize and remove stopwords/punctuation\n",
        "    doc = nlp(text)\n",
        "    clean_tokens = [token.text for token in doc if token.text not in stop_words and token.text not in string.punctuation]\n",
        "\n",
        "    return \" \".join(clean_tokens)\n",
        "\n",
        "# Example usage\n",
        "cleaned_resume = preprocess_text(resume_text)\n",
        "print(cleaned_resume)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdyuiluBfeW3",
        "outputId": "fe41b75f-2e68-42a5-bcbe-b43e6fcf4abe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fahad hossain \n",
            " /envel⌢pefahadhossain1139@gmail.com ♂ ¶ap-¶arker alt11355 sw sweetwater 2nd street miami florida \n",
            " ὑ7fahad16301139.github.io ♂ phone alt718 666 0971 \n",
            " education \n",
            " ms computer science florida international university jan 2023 – dec 2024 \n",
            " cgpa 3.74 \n",
            " b.sc computer science engineering brac university jan 2016 – dec 2020 \n",
            " cgpa 3.20 \n",
            " work experience \n",
            " teaching assistant florida international university miami jan 2024 present \n",
            " -led interactive lab sessions guiding students hands experience system level programming os con- \n",
            " cepts \n",
            " assisted students understanding practical applications programming real life scenarios potential career \n",
            " paths \n",
            " conducted tutorials bolster programming proficiency problem solving skills \n",
            " data analyst florida international university miami oct dec 2023 \n",
            " analyzed vs cs students ’ work experiences outline career paths using python \n",
            " presented findings team supervisor offering insights unique career trajectories opportunities associ- \n",
            " ated degree \n",
            " utilized postgresql power bi python data processing visualization interpretation \n",
            " volunteer research assistant florida international university may jul 2023 \n",
            " led xgboost gru project anomaly detection 96.57 accuracy innovated feature extraction techniques \n",
            " machine learning models \n",
            " thesis research assistant brac university dhaka feb dec 2020 \n",
            " developed machine learning algorithms team contributing ieee publication findings method- \n",
            " ologies \n",
            " projects \n",
            " calorie estimation system \n",
            " led development deep learning system using cnn resnet inception v3 food detection calorie esti- \n",
            " mation demonstrated potential health nutrition project link /external link alt \n",
            " car rental system \n",
            " developed php mysql based system streamline car rental processes users administrators project link /external link alt \n",
            " gunshot detector app \n",
            " developed innovative flask based api real time gunshot detection using machine learning feature extraction \n",
            " pre trained model featuring user friendly interface audio file uploads immediate results project link /external link alt \n",
            " publications \n",
            " foodiecal convolutional neural network based food detection calorie estimation system \n",
            " s. a. ayon c. z. mashrafi a. b. yousuf f. hossain m. i. hossain 2021 national computing colleges conference nccc \n",
            " 2021 pp 1 6 doi 10.1109 nccc49330.2021.9428820 /external link alt \n",
            " hybrid machine learning approach improved ponzi scheme detection using advanced feature engineering \n",
            " hossain fahad uddin jia florida international university department computer information science status \n",
            " submission \n",
            " skills \n",
            " programming languages java php sql python html css \n",
            " digital skills microsoft word powerpoint excel latex google colab \n",
            " fahad hossain page 1 1last updated february 2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_description = \"\"\"We are looking for a Data Scientist with 3+ years of experience in Python, Machine Learning, and Data Analysis...\"\"\"\n",
        "cleaned_job_description = preprocess_text(job_description)\n",
        "print(cleaned_job_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-zLkdu9fs3J",
        "outputId": "23650e68-fad8-46e4-e406-1292a3e4e140"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looking data scientist 3 years experience python machine learning data analysis ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def vectorize_texts(resume, job_desc):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectors = vectorizer.fit_transform([resume, job_desc])\n",
        "    return vectors\n",
        "\n",
        "resume_vector, job_desc_vector = vectorize_texts(cleaned_resume, cleaned_job_description)\n"
      ],
      "metadata": {
        "id": "4gVY_9rCfwrP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_score = cosine_similarity(resume_vector, job_desc_vector)\n",
        "print(f\"Resume-Job Description Similarity Score: {similarity_score[0][0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kWH3TjSgDZ3",
        "outputId": "c85fc584-1edf-471c-c96d-3ac63a6cf29e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume-Job Description Similarity Score: 0.11571462736618796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp4DdSwpf1Cs",
        "outputId": "e1597dfc-a690-40aa-fd2a-ce3968babeba"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.38.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.38.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, tenacity, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.38.0 tenacity-8.5.0 watchdog-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import PyPDF2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract resume text from a PDF\n",
        "def extract_resume_text(file):\n",
        "    reader = PyPDF2.PdfReader(file)\n",
        "    text = \"\"\n",
        "    for page in range(len(reader.pages)):\n",
        "        text += reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "# Preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    doc = nlp(text)\n",
        "    clean_tokens = [token.text for token in doc if token.text not in stop_words and token.text not in string.punctuation]\n",
        "    return \" \".join(clean_tokens)\n",
        "\n",
        "# Vectorize text and calculate cosine similarity\n",
        "def calculate_similarity(resume, job_desc):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectors = vectorizer.fit_transform([resume, job_desc])\n",
        "    similarity_score = cosine_similarity(vectors)[0][1]\n",
        "    return similarity_score\n",
        "\n",
        "# Streamlit App\n",
        "st.title(\"Resume Classifier and Job Matching System\")\n",
        "\n",
        "# Upload resume\n",
        "uploaded_file = st.file_uploader(\"Upload your Resume (PDF)\", type=\"pdf\")\n",
        "\n",
        "# Job description input\n",
        "job_desc = st.text_area(\"Paste Job Description\", height=150)\n",
        "\n",
        "# If both resume and job description are provided\n",
        "if uploaded_file and job_desc:\n",
        "    resume_text = extract_resume_text(uploaded_file)\n",
        "    cleaned_resume = preprocess_text(resume_text)\n",
        "    cleaned_job_description = preprocess_text(job_desc)\n",
        "\n",
        "    similarity_score = calculate_similarity(cleaned_resume, cleaned_job_description)\n",
        "\n",
        "    # Display similarity score\n",
        "    st.write(f\"Resume-Job Description Similarity Score: {similarity_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqeYn9lXgZKO",
        "outputId": "43eac245-c398-4241-b12c-57163673bc21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2024-09-24 00:50:04.375 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-24 00:50:04.606 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-09-24 00:50:04.618 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-24 00:50:04.626 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-24 00:50:04.629 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-24 00:50:04.633 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-24 00:50:04.638 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-24 00:50:04.643 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-24 00:50:04.644 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-24 00:50:04.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-24 00:50:04.649 Session state does not function when running a script without `streamlit run`\n",
            "2024-09-24 00:50:04.652 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-24 00:50:04.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZBrFu1SggYx",
        "outputId": "53b6f7c0-e8cc-4511-cf19-cf8b251237b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken cr_2mUlcTcyRemtKUFd8XUTMFyjUHN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igpP9NnvkXZu",
        "outputId": "34c31464-6f22-44b4-aeb0-8567b92f98c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2mUoiWYULIxq7xds0QdQl3Xd0wa_7SaaUsUxNKBchM5K2Uqwf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF9NrzIekeWC",
        "outputId": "a666e03d-7cdf-4d23-8435-630c17a5f24d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Streamlit app\n",
        "!streamlit run app.py &>/dev/null&\n",
        "\n",
        "# Expose the port explicitly using http\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(f\"Streamlit app is live at: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGK3HMPQm3VR",
        "outputId": "b8fdb69e-57e8-4c5a-b3a0-ea6d9c52ff8a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://2c00-34-150-179-246.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dI-HKW5oMmb",
        "outputId": "4a75c180-f73f-46fd-b7bd-040ac150e079"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       11117  0.0  0.0   6484  2308 ?        S    01:23   0:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 &>/dev/null&\n"
      ],
      "metadata": {
        "id": "XwRvEqRUoSTC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVBE6LHbnEm0",
        "outputId": "d3b4041d-d388-4e9b-d87d-6ec8d2c78f65"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       11254  0.0  0.0   6484  2256 ?        S    01:24   0:00 grep streamlit\n"
          ]
        }
      ]
    }
  ]
}